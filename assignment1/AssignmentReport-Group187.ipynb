{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1 Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an outline for your report to ease the amount of work required to create your report. Jupyter notebook supports markdown, and I recommend you to check out this [cheat sheet](https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet). If you are not familiar with markdown.\n",
    "\n",
    "Before delivery, **remember to convert this file to PDF**. You can do it in two ways:\n",
    "1. Print the webpage (ctrl+P or cmd+P)\n",
    "2. Export with latex. This is somewhat more difficult, but you'll get somehwat of a \"prettier\" PDF. Go to File -> Download as -> PDF via LaTeX. You might have to install nbconvert and pandoc through conda; `conda install nbconvert pandoc`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## task 1a)\n",
    "\n",
    "$$\n",
    "C^n(w)=-\\left(y^n \\ln \\left(\\hat{y}^n\\right)+\\left(1-y^n\\right) \\ln \\left(1-\\hat{y}^n\\right)\\right)\n",
    "$$\n",
    "\n",
    "Using that $y \\neq y(w_i)$:\n",
    "$$\n",
    "\\frac{\\partial C^n(w)}{\\partial w_i} = -y \\frac{\\partial \\ln \\hat{y}}{\\partial w_i} - (1-y) \\cdot \\frac{\\partial \\ln(1-\\hat{y})}{\\partial w_i}\n",
    "$$\n",
    "\n",
    "First term, using the chain-rule and the hint $\\frac{\\partial f\\left(x^n\\right)}{\\partial w_i}=x_i^n f\\left(x^n\\right)\\left(1-f\\left(x^n\\right)\\right)$: \n",
    "$$\n",
    "\\frac{\\partial \\ln \\hat{y}^n}{\\partial w_i} = \\frac{1}{\\hat{y}^n} \\cdot \\frac{\\partial \\hat{y}^n}{\\partial w_i} = \\frac{1}{\\hat{y}^n} \\cdot x_i^n \\cdot \\hat{y}^n \\cdot (1-\\hat{y}^n) = \\underline{\\underline{ x_i^n \\, (1-\\hat{y}^n) }}\n",
    "$$\n",
    "Second term, using the chain-rule and the hint $\\frac{\\partial f\\left(x^n\\right)}{\\partial w_i}=x_i^n f\\left(x^n\\right)\\left(1-f\\left(x^n\\right)\\right)$: \n",
    "$$\n",
    "\\frac{\\partial \\ln(1-\\hat{y}^n)}{\\partial w_i} = -\\frac{1}{1-\\hat{y}^n} \\cdot \\frac{\\partial (1-\\hat{y}^n)}{\\partial w_i} = -\\frac{1}{1-\\hat{y}^n} \\cdot x_i^n \\cdot (1-\\hat{y}^n) \\cdot (1-1+\\hat{y}^n) = \\underline{\\underline{ -x_i^n \\hat{y}^n }}\n",
    "$$\n",
    "\n",
    "\n",
    "Put together:\n",
    "$$\n",
    "\\frac{\\partial C^n}{\\partial w_i} = -y^n x_i^n (1-\\hat{y}^n) + (1-y^n) x_i^n \\hat{y}^n = x_i^n (-y^n + y^n\\hat{y}^n+\\hat{y}^n-y^n\\hat{y}^n) = x_i^n(\\hat{y}^n-y^n)= \\underline{\\underline{ -\\left(y^n-\\hat{y}^n\\right) x_i^n }} \\,\\,, \\quad q.e.d.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2b)\n",
    "![](task2b_binary_train_loss.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2c)\n",
    "![](RR_task2b_binary_train_accuracy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2d)\n",
    "Early stopping in Epoch 33."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2e)\n",
    "You\n",
    "should notice that the validation accuracy has fewer ”spikes”. Why does this happen?\n",
    "\n",
    "- By shuffling the dataset before each epoch, the data to train the model is more randomly distributed preventing possible correlations given by the order of the samples in the original dataset.\n",
    "\n",
    "![](task2e_train_accuracy_shuffle_difference.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3b)\n",
    "![](RR_task3b_softmax_train_loss.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3c)\n",
    "![](RR_task3c_softmax_train_accuracy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3d)\n",
    "At the start of the training the validation accuracy is higher than the training accuracy. Then the training accuracy increases more and more while the validation accuracy plateaus. This is an indicator for overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4a)\n",
    "\n",
    "$$ J(w)=C(w)+\\lambda R(w), \\qquad R(w)=\\|w\\|^2=\\sum_{i, j} w_{i, j}^2$$\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial w} = \\frac{\\partial C}{\\partial w} + \\frac{\\partial}{\\partial w} \\left( \\lambda \\, R(w) \\right)$$\n",
    "\n",
    "Using from before: $\\frac{\\partial C^n}{\\partial w_{kj}} = -x_j^n \\cdot (y_k^n - \\hat{y}_k^n),$\n",
    "and $\\frac{\\partial}{\\partial w} \\left( \\lambda \\, R(w) \\right) = 2\\lambda w$ .\n",
    "\n",
    "$$\\underline{\\underline{ \\frac{\\partial J}{\\partial w_{kj}} =  -x_j^n \\cdot (y_k^n - \\hat{y}_k^n) + 2\\lambda w_{kj} }}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4b)\n",
    "\n",
    "First row: $\\lambda=0$, second row: $\\lambda=1$. Why are the weights for the model with λ = 1.0 less noisy?\n",
    "\n",
    "Regularization is leading that the overall squared weight sum is leading to zero and therefore the absolute weights become smaller. However, to reduce the cost function, the right weights still need to be active in order to reduce the classification error. Combining both effects (right weights active, everything else going to zero), leading to significantly smaller noises for the weights.\n",
    "\n",
    "![](task4b_softmax_weight.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4c)\n",
    "![](task4c_l2_reg_accuracy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4d)\n",
    "You will notice that the validation accuracy degrades when applying any amount of regularization. What do you think is the reason for this?\n",
    "\n",
    "With too strong regularization, the model will tend to underfitting so the model complexity and the validation accuracy decreases. Common regularization parameters often range between 0 and 0.1.\n",
    "\n",
    "In addition, the accuracy only take into account the percentage of total correct predictions and might not be the correct metric. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4e)\n",
    "What do you observe?\n",
    "\n",
    "For a higher regularization factor $\\lambda$ the L2-norm of the weights become smaller, which is exspected since $\\lambda$ controls how much the L2-norm is weighted in the cost function. For bigger $\\lambda$ the L2-norm term becomes more important in the cost function and therefore will become smaller in order to minimize the lost function.\n",
    "\n",
    "![](task4d_l2_reg_norms.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
