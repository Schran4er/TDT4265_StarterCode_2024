{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1 Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an outline for your report to ease the amount of work required to create your report. Jupyter notebook supports markdown, and I recommend you to check out this [cheat sheet](https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet). If you are not familiar with markdown.\n",
    "\n",
    "Before delivery, **remember to convert this file to PDF**. You can do it in two ways:\n",
    "1. Print the webpage (ctrl+P or cmd+P)\n",
    "2. Export with latex. This is somewhat more difficult, but you'll get somehwat of a \"prettier\" PDF. Go to File -> Download as -> PDF via LaTeX. You might have to install nbconvert and pandoc through conda; `conda install nbconvert pandoc`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## task 1a)\n",
    "\n",
    "![task 1 a handwritten](task1a.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## task 1b)\n",
    "iii) Max pooling reduces the sensitivity to a small translational pixel shift since the maximum value of a small area is taken, which is robust as long the translational shift is small.\n",
    "\n",
    "\n",
    "## task 1c)\n",
    "using formula $W_2=\\left[\\left(W_1-F_W+2 P_W\\right) / S_W\\right]+1$ and $F_W=7, S_W=1$ we obtain:\n",
    "\n",
    "$P_w=\\frac{1}{2}\\left(S_w \\cdot\\left(w_n-1\\right)-w_1+F_ w\\right)=\\frac{1}{2}(F_ w-1)=3$\n",
    "\n",
    "Since the kernel is square, vice versa for $H_2$ and the required padding for each side is 3.\n",
    "\n",
    "\n",
    "## task 1d)\n",
    "using the same equation as in 1c) and using that everything is square / quadratic, so W=H, we get:\n",
    "\n",
    "$W_2 = \\frac{W_1-F+2P}{S}+1, F=S(1-W_2)+W_1-2P=1-W_2+W_1=5,$\n",
    "\n",
    "with $W_1=512, W_2=508, P=0, S=1$. So the Kernel size is 5x5.\n",
    "\n",
    "\n",
    "## task 1e)\n",
    "using the same equation and $F=2, P=0, S=2$:\n",
    "\n",
    "$W_3 = \\frac{W_2-F+2P}{S}+1 = 254$\n",
    "\n",
    "So the pooled features maps are of size 254x254.\n",
    "\n",
    "\n",
    "## task 1f)\n",
    "Using the same equation and $F=3, P=0, S=1$:\n",
    "\n",
    "$W_4 = \\frac{W_3-F+2P}{S}+1 = 252$\n",
    "\n",
    "So the feature maps after convolution in the second layer have the size 252x252.\n",
    "\n",
    "\n",
    "## task 1g)\n",
    "![task 1 g handwritten](task1g.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2\n",
    "\n",
    "### Task 2a)\n",
    "\n",
    "![task2a](../plots/task2_plot.png)\n",
    "\n",
    "\n",
    "### Task 2b)\n",
    "(see graph at Task 2a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3a)\n",
    "optimizer: Adam optimzer\n",
    "\n",
    "learning rate: 0.001\n",
    "\n",
    "batch size: 64\n",
    "\n",
    "no special weight initialization and no regularization as the model achieved 80% before we came to considering these \n",
    "additions\n",
    "\n",
    "early_stop_count = 4 \n",
    "\n",
    "stride = 1; pooling_stride = 4; padding = 1\n",
    "\n",
    "see task3.py for reference\n",
    "| Layer | LayerType   | Number of Hidden Units/Filters | Activation function |\n",
    "|-------|-------------|--------------------------------|---------------------|\n",
    "| 1     | Conv2D      | 64                             | LeakyReLU           |\n",
    "| 1     | MaxPool2d   | -                              | -                   |\n",
    "| 1     | Conv2D      | 64                             | LeakyReLU           |\n",
    "| 1     | MaxPool2d   | -                              | -                   |\n",
    "| 1     | BatchNorm2d | 64                             | -                   |\n",
    "| 2     | Conv2D      | 128                            | LeakyReLU           |\n",
    "| 2     | MaxPool2d   | -                              | -                   |\n",
    "| 2     | Conv2D      | 128                            | LeakyReLU           |\n",
    "| 2     | MaxPool2d   | -                              | -                   |\n",
    "| 2     | BatchNorm2d | 128                            | -                   |\n",
    "| 3     | Conv2D      | 256                            | LeakyReLU           |\n",
    "| 3     | MaxPool2d   | -                              | -                   |\n",
    "| 3     | Conv2D      | 256                            | LeakyReLU           |\n",
    "| 3     | MaxPool2d   | -                              | -                   |\n",
    "| 3     | BatchNorm2d | 256                            | -                   |\n",
    "| 4     | Conv2D      | 1024                           | LeakyReLU           |\n",
    "| 4     | MaxPool2d   | -                              | -                   |\n",
    "| 4     | Conv2D      | 1024                           | LeakyReLU           |\n",
    "| 4     | MaxPool2d   | -                              | -                   |\n",
    "| 4     | BatchNorm2d | 1024                           | -                   |\n",
    "| 5     | Linear      | 1024                           |                     |\n",
    "\n",
    "\n",
    "### Task 3b)\n",
    "![task2a](../plots/task3_plot_ALL.png)\n",
    "\n",
    "### Task 3c)\n",
    "The biggest improvement was observed in the last step of the incremental changes. So the change of the optimizer together with a different learning rate and adding batch normalization after each pooling layer made the biggest improvement. Batch normalization can help greatly as normalizing the data e.g. helps with the vanishing gradient problem. \n",
    "\n",
    "Using LogSigmoid as an activation function did not work good at all, different cases requires different activation functions, so it seems LogSigmoid is just a bad activation function for this particular task, architecture and dataset.\n",
    "\n",
    "### Task 3d)\n",
    "![task2a](../plots/task3_plot.png)\n",
    "\n",
    "### Task 3e)\n",
    "The final model \"3 vii\" already did reach an accuracy of 80% (see previous plot)\n",
    "\n",
    "### Task 3f)\n",
    "The loss curve for the final model \"3 vii\" gets slightly bigger after \"initially converging\", this is an indicator for overfitting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4a)\n",
    "\n",
    "Hyperparameters:\n",
    "- batch size = 32\n",
    "- learning rate = 5E-4\n",
    "\n",
    "\n",
    "Other settings:\n",
    "- Optimizer: Adam\n",
    "- use data augmentation, since it slighty improved the accuracy\n",
    "- use transforms.Resize !\n",
    "- changed mean and std to those of ImageNet dataset !\n",
    "\n",
    "Resulting final accuracy of <b>90.4%</b>\n",
    "\n",
    "\n",
    "![task4](../plots/task4_plot.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.1 64-bit ('py38': conda)",
   "language": "python",
   "name": "python38164bitpy38condac1f68ca5407a4349b0d7e37676f2fbb3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
