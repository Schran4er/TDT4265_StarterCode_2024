Task 2: Medical Image Segmentation

Cybele lab hours:
Mondays 15:15 - 17:00 and
Thursdays 12:00 - 14:00


QUESTIONS:
- pipeline is working, results are bad
https://wandb.ai/tdt4265-group187/ASOCA/reports/Untitled-Report--Vmlldzo3MzMzNDU4

- implement usage of pretrained model in our pipeline

-> Monai NN UNET!

- Pre-processing:
    - is our preproc ok?
    - Spacingd required? for isotropic images?! (but keep high resolution) Or use Resized or SpatialPadd? --> Trying out!
    - Add Frangi filter or other filter for detection of tubular structures? (optional: pretrained NN)
- Training:
    - DiceLoss, DiceMetric, Adam optimizer ok?
- Post-processing:
    - how important is post-processing?
    - filter or image processing operations?
    - pretrained NN needed?



---

Morgen:
wie ist tutorial Stand? 
    https://www.youtube.com/watch?v=8ZCWLC3xI_Q
    https://docs.monai.io/en/stable/ (getting started)
    https://github.com/Project-MONAI/tutorials (3d segmentation)
    https://github.com/Project-MONAI/tutorials/blob/main/3d_segmentation/spleen_segmentation_3d.ipynb


zuerst simple architektur 
    -> refinement
    - ✅ test cuda in notebook on idun, login node? gpu node? interactive slurm job? https://www.hpc.ntnu.no/idun/

Todos:
- code und ordner aufräumen!!
- preprocessing schreiben! (felix)
- Do we need to detect/classify between normal/diseased or just segmentate them all

Plan:
 - Data exploration: nrrd, vtp, stl data ✅
        Diseased/Normal
        ├── Annotations --> images segmentated by the 3 experts manually -> ground truth!
        ├── CTCA --> whole scans (2D) / raw data! goal: detect aorta, generate 3d (centerline & surfacemeshes), classify normal/diseased
        ├── Centerlines --> centerlines of aortas
        ├── SurfaceMeshes --> aorta surfaces
        └── Testset_Disease/Testset_Normal -> useless because not labelled

- see MONAI ✅

- Pre-processing pipeline:
    - keep in mind that different .nrrd files may have differenz z-dimension -> SpacingD correct pixdim ✅
    -> divide into training/test/val (because testset doesn't have labels) -> Felix
    -> ✅ set random seed (in monai) for comparable results
    -> Data augmentation: crop/rotate/flip/... (look at previous task (assignement 3))
    - Convert data to usable format for a ML model:
        - use 3D augmentation (https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9952534/ "Comparing 3D, 2.5D, and 2D Approaches to Brain Image Auto-Segmentation")

    - image contrast normalization to zero mean and unit variance (ASOCA paper submission 2)
    - Frangi/... filters not/? used (as ASOCA paper states that they are too computationally expensive)


- build baseline model
    - Look for pre trained model for medical image segmentation (from literature) which has not been trained on the asoca dataset!!
- build training setup!!
    - loss function: Soft Dice loss + Cross entropy loss or focal loss (tensorboard for keeping track of the loss?)
    - performance metrics
    - optimizer
    - regularization
    - early stopping
    - pre training (take pre trained model, adjust it, train on our data)
    - speed up training with batch normalization

- post-processsing: remove small disconnected components

- do improvements
    - note and justify/explain the import changes!!
- do the same with another architecture

- ensure that we only train on ONE gpu (-> TA session)

- runtime analysis
    - runtime = inference time: how fast can the model detect? (in real application)
- carbon footprint analysis
 - total training time * GPU_power = work -> convert to how far we can drive with a tesla
 - implement automated analysis. We use approx 13 hrs training for 1kWh, which equals 7.3km for Tesla Model 3 or 1.59 Big Macs


Documentation:
- Visualization: show annotation and AI segmentation result
- Readme for code (how to train, use the model etc., conda env)
- Technical documentation
- Presentation


Further work:
- other filters (check also other submissions from ASOCA paper)
- further post-processing magic (check also other submissions from ASOCA paper)



Dataset infos (nrrd):
     1 NRRD0005         -> NRRD version 5
     2 # This NRRD file was generated by pynrrd
     3 # on 2020-08-13 06:54:10(GMT).
     4 # Complete NRRD file format specification at:
     5 # http://teem.sourceforge.net/nrrd/format.html
     6 type: int32
     7 dimension: 3     -> 3 dims (up to 16 possible)
     8 space: left-posterior-superior       --> orientation of the nrrd grid relative to the surrounding space, e.g. patient. "left-posterior-superior" or "LPS":  For medical data, a patient-based right-handed coordinate frame, with ordered basis vectors pointing towards left, posterior, and superior, respectively. This space is used in DICOM 3.
     9 sizes: 512 512 224  -> number of samples along each dimension
    10 space directions: (0.37890600000000008,0,0) (0,0.37890600000000008,0) (0,0,0.62500000000000011)  -> axis directions in global coordinates
    11 kinds: domain domain domain
    12 endian: little
    13 encoding: gzip
    14 space origin: (-88.300003051757855,-128.80000305175787,-244.37500000000014)      -> global coordinates of first voxel


Mittwoch:
    - Preprocessing Grundmodell
    - Refactoring
    - fix Accuracy (-> look at inference output?)
    - Read into UNet:
        architecture with randomly initialized weights
        - related work research: pick a model (with random weights) as baseline, then the same but pretrained, then try different loss/acc functions

Donnerstag:
    - Modell mit vollständigem Datensatz laufen lassen:
        wenn gut: dann ist das baseline
            -> investigate data augmentation & postprocessing
        wenn schlecht: Übungsstunde
    - refactor -> getan
    - implement pretrained model

bis zur Übungsstunde:
    - Felix: Pre- & Postprocessing
    - Alex: pretrained model zum laufen bringen





-----------------------------------------------------

06.04.: 
/cluster/work/felixzr/TDT4265_StarterCode_2024/project/model1_auto3dseg
^beinhält was ich gemacht hab, Kurzfassung: Die model1_auto3seg_main.py startet bei Ausführung ein Training für 3 unterschiedliche Modelle nacheinander für jeweils die angegebene Anzahl an Epochen
die 3 Modelle sind 'dints', 'segresnet' (segresnet2d wird bei uns nicht trainiert, weil unsere Daten 3d sind) und 'swinunetr' 

Loss bzw. acc kann mit den tensorboard Events files auslesen, siehe dazu bei Bedarf https://pytext.readthedocs.io/en/master/visualize_your_model.html


herauszufinden ist: [in den Ordnern sind teilweise docu files drin, ansonsten auch ]
- [ ] ÜbungsTAs fragen: reicht das schon als "Implement at least two architectures."?
- [ ] Welche Sachen kann man manuell anpassen? Damit man eine Baseline und iwas was man selber angepasst hat, hat
    -> unter anderem, kann man wohl 2 unterschiedliche Module einbinden, die extra hyperparameter optimieren (siehe Doku online)
- [ ] Welcher loss/metric wird da eig. genau verwendet, kann man das anpassen?
- [] lassen sich die Sachen einzeln trainieren? (wenn ja: dann jedes mal für paar Stunden laufen lassen)
    -> läuft gerade für swinunetr_4
        -> ist abgebrochen weil CUDA OutOfMemory, allerdings weird, weil davor hat's ja auch trainiert
    -> segresnet_0 ist durchgelaufen, hat für 25 Epochen 04:23:49 gebraucht
- [ ] Wie funktionieren eig. die einzelnen Modelle die wir da trainiert haben?
- [x] wofür steht jeweils _x: 
    -> "The folder name suffix indicates the ith fold of N-fold cross-validation."


wie präsentieren wir die Ergebnisse? Wäre natürlich cool iwie die ground truth in 3d zu zeigen und dann das was das Model predicted hat (ansonsten das einfach als 2d Bilder; wenn die Ergebnisse shitty sind dann einfach: ja hätten's halt länger trainiert)


Präsi:
https://studntnu-my.sharepoint.com/:p:/g/personal/felixzr_ntnu_no/EU9Z_3owAk5JrNeTixtN7VYBcd5FhQ5CiyuOFs3bvQ__3Q?e=gSbvzC

Report:
https://www.overleaf.com/9632691999qwtbxyyjrshx#dc43ac


Paper Präsi:
https://docs.google.com/presentation/d/1D_uhf5act_6nipsR3hIWFuQf6EOoXUwZ/edit#slide=id.p1


Pipeline / readme:
- train: train.py...
- postproc.py



TODO:
model3dseg: 
dints0 -> Model A
segresnet0 -> Model B
- wie viele epochen trainieren? für < 12 h  ✅
    - dints0: 4.5 min / epoch -> max. 158 epochs
    - segresnet0: 1.55 min / epoch -> ergo max. 464 epochs

- imageCAS daten einlesen✅

- training einleiten: ✅
    - train on asoca data
    - pretrain! 12h on imageCAS, then pretrain 12h von ASOCA
        - herausfinden wie man training fortsetzt von best_metric_model.pt: auf imageCAS trainieren, dann hyper_parameters
            -> in hyper_parameter.yaml: fine tuning auf true setzen.


- postproc + evaluation: graphen (train+val loss&acc), mean dice & HD95 metrics
    - graphs!!! ✅
        - graphen (train+val loss&acc) 
        - table: mean dice & HD95 metrics -> Felix ✅
    - (run on test dataset: check inference time, graphs, metrics --> not needed, wird automatisch gemacht)
    - inference time ermitteln: model (model_fol0/best_metric_model.pt) in notebook einlesen, preproc+forward pass durchführen ✅
    --> script: felix ✅
    dints0: model/best_metric_model.pt ist bestes model, model/current_model.pt ist aktuelles model
    segresnet0: model/model.pt ist bestes model, model/model_final.pt ist aktuelles model

- optional: show the 3d model of best and worst performance -> Felix


Zeitplan:
- Di abend Paper Präsi fertig + Aufnahme Paper Alex fertig ✅
- Mi abend Projekt Präsi inhaltlich fertig (✅)
- Do nachmittag aufnahme (alex) fertig ✅



Presentation:
- rausfinden, was für preproc model3dseg macht! ✅
    --> /cluster/work/felixzr/TDT4265_StarterCode_2024/project/model1_auto3dseg/dints_0/configs/transforms_train.yaml
- dints & segresnet architektur: alex ✅
- inference time:
    - dints0: 80.56 s per dataset ✅
    - segresnet0: 6.99 s per dataset ✅
    - dints with Pretrain: 81.1 s per dataset ✅
    - segresnet with Pretrain: 6.91 s per dataset ✅

AM ENDE: Über aufagbenstellung gehen und alles abhaken ✅